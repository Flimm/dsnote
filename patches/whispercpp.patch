diff -ruN a/ggml.c b/ggml.c
--- a/ggml.c	2023-02-28 21:29:12.000000000 +0100
+++ b/ggml.c	2023-03-21 18:22:56.350018585 +0100
@@ -1279,6 +1279,7 @@
     size_t mem_size;
     void * mem_buffer;
     bool   mem_buffer_owned;
+    bool   stop;
 
     int n_objects;
 
@@ -1548,6 +1549,7 @@
         /*.mem_size         =*/ params.mem_size,
         /*.mem_buffer       =*/ params.mem_buffer ? params.mem_buffer : malloc(params.mem_size),
         /*.mem_buffer_owned =*/ params.mem_buffer ? false : true,
+        /*.stop             =*/ false,
         /*.n_objects        =*/ 0,
         /*.objects_begin    =*/ NULL,
         /*.objects_end      =*/ NULL,
@@ -1564,6 +1566,10 @@
     return ctx;
 }
 
+void ggml_cancel(struct ggml_context * ctx) {
+    ctx->stop = true;
+}
+
 void ggml_free(struct ggml_context * ctx) {
     // make this function thread safe
     ggml_critical_section_start();
@@ -7474,7 +7480,7 @@
     const int64_t perf_start_cycles  = ggml_perf_cycles();
     const int64_t perf_start_time_us = ggml_perf_time_us();
 
-    for (int i = 0; i < cgraph->n_nodes; i++) {
+    for (int i = 0; i < cgraph->n_nodes && !ctx->stop; i++) {
         GGML_PRINT_DEBUG_5("%s: %d/%d\n", __func__, i, cgraph->n_nodes);
 
         struct ggml_tensor * node = cgraph->nodes[i];
@@ -7504,7 +7510,7 @@
                 atomic_store(&state_shared.has_work, false);
             }
 
-            while (atomic_load(&state_shared.has_work)) {
+            while (atomic_load(&state_shared.has_work) && !ctx->stop) {
                 ggml_lock_lock  (&state_shared.spin);
                 ggml_lock_unlock(&state_shared.spin);
             }
@@ -7523,7 +7529,7 @@
 
             atomic_fetch_sub(&state_shared.n_ready, 1);
 
-            while (atomic_load(&state_shared.n_ready) > 0) {
+            while (atomic_load(&state_shared.n_ready) > 0 && !ctx->stop) {
                 ggml_lock_lock  (&state_shared.spin);
                 ggml_lock_unlock(&state_shared.spin);
             }
@@ -7540,14 +7546,14 @@
                 atomic_store(&state_shared.has_work, false);
             }
 
-            while (atomic_load(&state_shared.has_work)) {
+            while (atomic_load(&state_shared.has_work) && !ctx->stop) {
                 ggml_lock_lock  (&state_shared.spin);
                 ggml_lock_unlock(&state_shared.spin);
             }
 
             atomic_fetch_sub(&state_shared.n_ready, 1);
 
-            while (atomic_load(&state_shared.n_ready) != 0) {
+            while (atomic_load(&state_shared.n_ready) != 0 && !ctx->stop) {
                 ggml_lock_lock  (&state_shared.spin);
                 ggml_lock_unlock(&state_shared.spin);
             }
@@ -7559,7 +7565,7 @@
                 atomic_store(&state_shared.has_work, false);
             }
 
-            while (atomic_load(&state_shared.has_work)) {
+            while (atomic_load(&state_shared.has_work) && !ctx->stop) {
                 ggml_lock_lock  (&state_shared.spin);
                 ggml_lock_unlock(&state_shared.spin);
             }
@@ -7578,7 +7584,7 @@
 
             atomic_fetch_sub(&state_shared.n_ready, 1);
 
-            while (atomic_load(&state_shared.n_ready) > 0) {
+            while (atomic_load(&state_shared.n_ready) > 0 && !ctx->stop) {
                 ggml_lock_lock  (&state_shared.spin);
                 ggml_lock_unlock(&state_shared.spin);
             }
@@ -7595,14 +7601,14 @@
                 atomic_store(&state_shared.has_work, false);
             }
 
-            while (atomic_load(&state_shared.has_work)) {
+            while (atomic_load(&state_shared.has_work) && !ctx->stop) {
                 ggml_lock_lock  (&state_shared.spin);
                 ggml_lock_unlock(&state_shared.spin);
             }
 
             atomic_fetch_sub(&state_shared.n_ready, 1);
 
-            while (atomic_load(&state_shared.n_ready) != 0) {
+            while (atomic_load(&state_shared.n_ready) != 0 && !ctx->stop) {
                 ggml_lock_lock  (&state_shared.spin);
                 ggml_lock_unlock(&state_shared.spin);
             }
diff -ruN a/ggml.h b/ggml.h
--- a/ggml.h	2023-02-28 21:29:12.000000000 +0100
+++ b/ggml.h	2023-03-21 17:51:01.109740644 +0100
@@ -331,6 +331,7 @@
 
 struct ggml_context * ggml_init(struct ggml_init_params params);
 void ggml_free(struct ggml_context * ctx);
+void ggml_cancel(struct ggml_context * ctx);
 
 size_t ggml_used_mem(const struct ggml_context * ctx);
 
diff -ruN a/whisper.cpp b/whisper.cpp
--- a/whisper.cpp	2023-02-28 21:29:12.000000000 +0100
+++ b/whisper.cpp	2023-03-21 20:41:41.526666686 +0100
@@ -603,6 +603,9 @@
     // [EXPERIMENTAL] speed-up techniques
     int32_t exp_n_audio_ctx = 0; // 0 - use default
 
+    bool stop = false;
+    ggml_context * gctx = nullptr;
+
     void use_buf(struct ggml_context * ctx, int i) {
 #if defined(WHISPER_USE_SCRATCH)
         size_t last_size = 0;
@@ -1378,7 +1381,9 @@
     params.mem_buffer = wctx.buf_compute.data();
 
     struct ggml_context * ctx0 = ggml_init(params);
-
+    wctx.gctx = ctx0;
+    if (wctx.stop) ggml_cancel(ctx0);
+        
     wctx.use_buf(ctx0, 0);
 
     struct ggml_tensor * mel = ggml_new_tensor_2d(ctx0, GGML_TYPE_F32, 2*n_ctx, n_mels);
@@ -1767,7 +1772,7 @@
         ggml_graph_compute(ctx0, &gf);
         //ggml_graph_print(&gf);
     }
-
+    
     ////////////////////////////////////////////////////////////////////////////
 
     //printf("%s: used_mem = %f MB, %f MB, %f MB %f MB %f MB\n", __func__,
@@ -1777,6 +1782,7 @@
     //        wctx.get_buf_max_mem(2)/1024.0/1024.0,
     //        wctx.get_buf_max_mem(3)/1024.0/1024.0);
 
+    wctx.gctx = nullptr;
     ggml_free(ctx0);
 
     wctx.t_encode_us += ggml_time_us() - t_start_us;
@@ -3488,6 +3494,11 @@
     }
 }
 
+void whisper_cancel(struct whisper_context * ctx) {
+    ctx->stop = true;
+    if (ctx->gctx) ggml_cancel(ctx->gctx);
+}
+
 int whisper_full(
         struct whisper_context * ctx,
         struct whisper_full_params params,
@@ -3681,6 +3692,8 @@
             return -6;
         }
 
+        if (ctx->stop) return -100;
+
         // if there is a very short audio segment left to process, we remove any past prompt since it tends
         // to confuse the decoder and often make it repeat or hallucinate stuff
         if (seek > seek_start && seek + 500 >= seek_end) {
@@ -3764,6 +3777,8 @@
                     return -7;
                 }
 
+                if (ctx->stop) return -100;
+
                 {
                     const int64_t t_start_sample_us = ggml_time_us();
 
@@ -4002,6 +4017,8 @@
                         return -8;
                     }
 
+                    if (ctx->stop) return -100;
+
                     {
                         const int64_t t_start_sample_us = ggml_time_us();
 
@@ -4200,6 +4217,8 @@
             WHISPER_PRINT_DEBUG("seek = %d, seek_delta = %d\n", seek, seek_delta);
         }
     }
+    
+    if (ctx->stop) return -100;
 
     return 0;
 }
diff -ruN a/whisper.h b/whisper.h
--- a/whisper.h	2023-02-28 21:29:12.000000000 +0100
+++ b/whisper.h	2023-03-21 17:47:39.818480170 +0100
@@ -341,6 +341,9 @@
                            const float * samples,
                                    int   n_samples);
 
+    // Cancel any ongoing operation.
+    WHISPER_API void whisper_cancel(struct whisper_context * ctx);
+
     // Split the input audio in chunks and process each chunk separately using whisper_full()
     // It seems this approach can offer some speedup in some cases.
     // However, the transcription accuracy can be worse at the beginning and end of each chunk.
